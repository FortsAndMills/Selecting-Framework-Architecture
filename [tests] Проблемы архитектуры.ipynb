{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выбор архитектуры!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, передо мной встал вопрос о более грамотной архитектуре моего RL-кода.\n",
    "\n",
    "Основные концепции:\n",
    "* модульность - основное требование. Агент должен собираться из модулей как из кубиков лего.\n",
    "* модификация структуры алгоритма должна выражаться в виде модуля. if double_dqn: do_one_thing() else: do_another_thing() здесь не выживет.\n",
    "* в частности, у пользователя должна быть возможность в две строчки подменить какой-нибудь метод агента (например, лосс-функцию).\n",
    "* я осознаю, что в питоне можно сделать всё, но нужно адекватное, чистое и элегантное решение. Разрешается, при необходимости, спрятать требуемую мутотень в условно базовый класс, если этим будет удобно пользоваться в том числе при создании новых модулей.\n",
    "\n",
    "[внимание, код далее условный и не предназначен для запуска]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "CartpoleNN = nn.Sequential(\n",
    "                nn.Linear(4, 20),\n",
    "                nn.ELU(),\n",
    "                nn.Linear(20, 20),\n",
    "                nn.ELU())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вариант №1. Старый вариант"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Агент есть один класс.\n",
    "* Модули получаются за счёт динамического наследования друг от друга.\n",
    "* Полученный агент принимает все гиперпараметры на вход в виде конфиг-словаря"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# СКРЫТЫЕ ВНУТРЕННОСТИ АРХИТЕКТУРЫ:\n",
    "# отсутствуют!\n",
    "# Возможно добавить немного костылей, чтобы делать проверку, например, что все заданные в конфиги гиперпараметры\n",
    "# действительно используются и не произошло опечатки в названии (это местная проблема)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# это пишет пользователь, придумавший свою лосс-функцию\n",
    "def MyLoss(parclass):                        # соглашение: модуль может быть унаследован от произвольного класса\n",
    "    class MyLoss(parclass):        \n",
    "        def loss(self, prediction, truth):\n",
    "            return self.config[\"hp\"]         # соглашение: гиперпараметры хранятся в self.config\n",
    "    return MyLoss                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание агента\n",
    "config = {\n",
    "    \"env\": env,\n",
    "    \"network\": CartpoleNN,\n",
    "    \"buffer_size\": 10^4,\n",
    "    \"optimizer\": Adam,\n",
    "    \"target_update_frequency\": 100,\n",
    "    \"hp\": 42\n",
    "}\n",
    "\n",
    "Agent = Runner()\n",
    "Agent = Replay(Agent)\n",
    "Agent = DQN(Agent)                    # неявно создаётся голова нейросетки, оптимизатор, пайплайн обучения нейросети...\n",
    "Agent = Target(Agent)\n",
    "Agent = eGreedy(Agent)\n",
    "Agent = MyLoss(Agent)\n",
    "\n",
    "agent = Agent(config)\n",
    "agent.learn(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Недостатки (причины появления данного файла):\n",
    "\n",
    "0.1) \"неявное\" создание модулей.\n",
    "\n",
    "0.2) гиперпараметры модулей слились в одну кучу. Непонятно, к какому модулю какой гиперпараметр относится.\n",
    "\n",
    "1) очевидно, сделать в системе два DQN или два оптимизируемых функционала можно только через одно место.\n",
    "\n",
    "2) всё лежит в одном объекте и рискует перезаписать переменные предыдущих агентов\n",
    "\n",
    "3) пользователю может быть неочевидно, в каком порядке нужно перечислять модули"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вариант №1.1. Альтернативный старый вариант"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(+) минимальное количество строчек кода\n",
    "(+) аккуратный синтаксис\n",
    "(-) основные проблемы 1-3 предыдущего варианта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# СКРЫТЫЕ ВНУТРЕННОСТИ АРХИТЕКТУРЫ:\n",
    "# всё ещё отсутствуют!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# это пишет пользователь, придумавший свою лосс-функцию\n",
    "def MyLoss(parclass, hp=42):                       # соглашение: модуль может быть унаследован от произвольного класса\n",
    "    class MyLoss(parclass):\n",
    "        def loss(self, prediction, truth):\n",
    "            return hp\n",
    "    return MyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agent = Runner(env=env)\n",
    "Agent = Replay(Agent, buffer_size=10^4)\n",
    "Agent = Network(Agent, network=CartpoleNN, optimizer=Adam)\n",
    "Agent = DQN(Agent)\n",
    "Agent = Target(Agent, target_update_frequency=100)\n",
    "Agent = eGreedy(Agent)\n",
    "Agent = MyLoss(Agent, hp=42)\n",
    "\n",
    "agent = Agent()\n",
    "agent.learn(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вариант №затыка. Пытаемся разбить на отдельные блоки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы решить проблему, что все модули стакаются в один объект, сделаем так:\n",
    "* модули получают в качестве гиперпараметров ссылки на другие необходимые модули (зависимости от других модулей придётся указывать явно)\n",
    "* от модулей всё ещё можно получать новые модификации путём наследования\n",
    "\n",
    "Пробуем лобовой подход:\n",
    "* гиперпараметры передаём при создании класса\n",
    "* ссылки на другие модули (уже объекты классов) передаём в конструктор модуля\n",
    "\n",
    "Соответственно, перед передачей ссылки нужно создать модуль."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# СКРЫТЫЕ ВНУТРЕННОСТИ АРХИТЕКТУРЫ:\n",
    "# всё ещё отсутствуют!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# это пишет пользователь, придумавший свою лосс-функцию\n",
    "def MyLoss(parclass, hp=42):\n",
    "    class MyLoss(parclass):\n",
    "        def loss(self, prediction, truth):\n",
    "            return hp\n",
    "    return MyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# в нашем примере пусть будет 4 модуля: runner, replay, network, dqn\n",
    "runner = Runner(env=env)()\n",
    "replay = Replay(buffer_size=10^4)(runner)      # buffer_size - гиперпараметр, ссылка на модуль runner идёт в конструктор\n",
    "network = Network(optimizer=Adam)()\n",
    "\n",
    "dqn = DQN()                                    # пока это класс\n",
    "dqn = Target(dqn, target_update_frequency=100) # улучшаем класс\n",
    "dqn = MyLoss(dqn, hp=42)                       # ещё улучшаем\n",
    "dqn = dqn(replay, network)                     # вызываем конструктор, создавая модуль и передавая необходимые ссылки\n",
    "eGreedy = ?!?                                  # а вот и засада\n",
    "\n",
    "runner.run(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В чём здесь проблема: в рекурсивной зависимости. Runner-у должно быть принипиально пофиг, сколько ещё модулей есть в системе. При этом ему нужна стратегия (метод def act(self, s)), которым мы, собственно, играем в игры. От Runner-а зависит реплей буффер, от буффера DQN. Но затем нужно подцепить в Runner ссылку на DQN (а точнее даже как-то на eGreedy)...\n",
    "\n",
    "eGreedy можно в рамках концепции полагать или наследником DQN, или наследником Runner-а, но проблему это не решает. В первом случае непонятно, как обновить метод runner.act уже после создания runner-а (писать runner.act = dqn.act, очевидно, отвратительнейший вариант, и в более сложных алгоритмах подобные рекурсивные связи - частое явление (например, Twin DQN)). Во втором случае runner уже создан, и как унаследоваться от класса и элегантно \"обновить\" его экземпляр непонятно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вариант №2. Класс System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы решить проблему, делаем так:\n",
    "* все модули наследуются от базового класса RLmodule\n",
    "* класс System компонует модули в одну рабочую систему\n",
    "* от модулей всё ещё можно получать новые модификации путём наследования, зависимости от других модулей придётся указывать явно.\n",
    "\n",
    "Тогда необходимо предоставить интерфейс связывания модулей.\n",
    "\n",
    "Первый вариант получается немного упоротым."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# СКРЫТЫЕ ВНУТРЕННОСТИ АРХИТЕКТУРЫ:\n",
    "class System:\n",
    "    def __init__(self):\n",
    "        self.modules = []\n",
    "    \n",
    "    def add(self, module):\n",
    "        # module - класс (!), унаследованный от RLmodule\n",
    "        \n",
    "        # добавляет модуль в список модулей\n",
    "        self.modules += [module]\n",
    "        \n",
    "        # возвращает ID\n",
    "        return len(self.modules)\n",
    "    \n",
    "    def update(self, module_id, update, args):\n",
    "        # берёт модуль с ID=module (это класс), наследует от него update и кладёт по тому же ID.\n",
    "        \n",
    "    def create(self):\n",
    "        # инициализирует (вызывает конструкторы) все модули\n",
    "\n",
    "class RLmodule:\n",
    "    def __init__(self, system)\n",
    "        self.system = system\n",
    "        \n",
    "# зачем: способ обращения к другому модулю будет выглядеть тогда как-то так:\n",
    "# на примере вызова реплей-буффера из DQN:\n",
    "self.system[self.replay].sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# это пишет пользователь, придумавший свою лосс-функцию\n",
    "def MyLoss(parclass=RLmodule, hp=42):       # соглашение: модуль должен быть унаследован от RLmodule или производного\n",
    "    class MyLoss(parclass):\n",
    "        def loss(self, prediction, truth):\n",
    "            return hp\n",
    "    return MyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = System()\n",
    "runner = system.add(Runner(env=env))                                # Runner возвращает класс, унаследованный от RLmodule\n",
    "replay = system.add(Replay(runner=runner, buffer_size=10^4))        # Replay тоже, но ещё он запоминает ID runner-а\n",
    "network = system.add(Network(optimizer=Adam))                       \n",
    "dqn = system.add(DQN(replay, network))                              # DQN запоминает ID модулей replay, network в системе\n",
    "dqn = system.update(dqn, Target, {\"target_update_frequency\": 100})  # system.update наследует Target от DQN\n",
    "dqn = system.update(dqn, MyLoss, {\"hp\": 42})                        # тоже самое\n",
    "runner = system.update(runner, eGreedy, {\"greedy_agent\": dqn})      # тоже самое, но eGreedy теперь ещё нужно подсоединитсья к dqn\n",
    "\n",
    "agent = system.create()\n",
    "agent.learn(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(+) проблемы решены\n",
    "\n",
    "(-) system.add и system.update повсюду, причём нужно думать, что где ставить\n",
    "\n",
    "(-) разный синтаксис передачи гиперпараметров и подсоединений\n",
    "\n",
    "(-) непонятно, где подсоединяются модули, а где гиперпараметры\n",
    "\n",
    "(-) очень мутно и тяжеловесно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вариант №3. Процедура сборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# СКРЫТЫЕ ВНУТРЕННОСТИ АРХИТЕКТУРЫ:\n",
    "class System:\n",
    "    def create(self, modules):\n",
    "        # см. вызов в примере далее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# это пишет пользователь, придумавший свою лосс-функцию\n",
    "def MyLoss(parclass, hp=42):\n",
    "    class MyLoss(parclass):\n",
    "        def loss(self, prediction, truth):\n",
    "            return hp\n",
    "    return MyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = Runner(env=env)\n",
    "replay = Replay(buffer_size=10^4)\n",
    "network = Network(optimizer=\"Adam\")\n",
    "dqn = DQN()\n",
    "dqn = Target(dqn, target_update_frequency=100)\n",
    "dqn = MyLoss(dqn, hp=42)\n",
    "runner = eGreedy(runner)\n",
    "\n",
    "dqn = dqn()\n",
    "runner = runner()\n",
    "replay = replay()\n",
    "network = network()\n",
    "\n",
    "agent = System().create(\n",
    "    (runner, {\"dqn\": dqn})\n",
    "    (replay, {\"runner\": runner}),\n",
    "    (network, {}),\n",
    "    (dqn, {\"runner\": runner, \"network\": network})\n",
    ")\n",
    "agent.learn(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По сути, System делает сейчас что-то вроде такого: \\\n",
    "runner.dqn = dqn \\\n",
    "replay.runner = runner \\\n",
    "dqn.runner = runner \\\n",
    "dqn.network = network\n",
    "\n",
    "Это порешало многие проблемы, но процедура инициализации очень громоздкая.\n",
    "\n",
    "(-) вызов System.create костылющий\n",
    "\n",
    "(-) сначала блок создания классов, потом блок создания объектов, потом большой вызов System..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вариант №4. Сборка по связям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# СКРЫТЫЕ ВНУТРЕННОСТИ АРХИТЕКТУРЫ:\n",
    "class System:\n",
    "    def __init__(self, **kwargs):\n",
    "        # см. вызов в примере далее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пользователю становится тяжелее...\n",
    "def MyLoss(name, hp=42):\n",
    "    def MyLoss(parclass):\n",
    "        class MyLoss(parclass):\n",
    "            def __init__(self, system):\n",
    "                super().__init__(self, system, name)   # нужно указывать явно, чтобы передать name...\n",
    "            \n",
    "            def loss(self, prediction, truth):\n",
    "                return hp\n",
    "        return MyLoss\n",
    "    return MyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = System(\n",
    "    Runner(\"runner\", env=env),\n",
    "    eGreedy(\"runner\", dqn=\"dqn\"),\n",
    "    Replay(\"replay\", runner=\"runner\", buffer_size=10^4),\n",
    "    Network(\"network\", optimizer=Adam),\n",
    "    DQN(\"dqn\", runner=\"runner\", replay=\"replay\"),\n",
    "    Target(\"dqn\", target_update_frequency=100),\n",
    "    MyLoss(\"dqn\", hp=42)\n",
    ")\n",
    "\n",
    "agent.learn(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что делает System: для каждого аргумента наследует очередной элемент списка от предыдущего, если их имена совпадают, заменяет поля-токены соответственно именам, поданным в System.\n",
    "\n",
    "(-) отвратительнейшее оформление нового модуля (функция, возвращающая функцию, возвращающую класс + явный конструктор)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вариант №5. Искусственное наследование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не наследуем модули одни от других. Все модули просто унаследованы напрямую от RLmodule, и искусственный механизм наследования как-то (?) запихнут в System и RLmodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# СКРЫТЫЕ ВНУТРЕННОСТИ АРХИТЕКТУРЫ:\n",
    "class RLmodule:\n",
    "    def __init__(self, name):\n",
    "        self._name = name\n",
    "\n",
    "class System:\n",
    "    def __init__(self, **kwargs):\n",
    "        # см. вызов в примере далее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLoss(RLmodule):\n",
    "    def __init__(self, name, hp=42):\n",
    "        super().__init__(self, name)\n",
    "        self.hp = hp\n",
    "\n",
    "    def loss(self, prediction, truth):\n",
    "        return self.hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = System(\n",
    "    Runner(\"runner\", env=env),\n",
    "    Replay(\"replay\", runner=\"runner\", buffer_size=10^4),\n",
    "    Network(\"network\", optimizer=Adam),\n",
    "    DQN(\"dqn\", runner=\"runner\", replay=\"replay\"),\n",
    "    Target(\"dqn\", target_update_frequency=100),\n",
    "    MyLoss(\"dqn\", hp=42),\n",
    "    eGreedy(\"runner\", dqn=\"dqn\"),\n",
    ")\n",
    "\n",
    "agent.learn(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(+) получилась какая-то приемлемая внешность фреймворка\n",
    "\n",
    "(-) без наследования сами модули будут иметь кучу костылей. В частности, у них не будет (прямого) доступа к полям \"предков\", и это надо будет костылить в искусственном наследовании. И к тому, как его делать, тоже много вопросов :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ВЫВОДЫ:** один вариант хуже другого."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вариант №6: Смесь вариантов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class RLmodule():\n",
    "    def __init__(self, name, system):\n",
    "        self.system = system\n",
    "        self.name = name\n",
    "        \n",
    "    def EMIT(self, message, *args, **kwargs):\n",
    "        self.system.SEND(self.name, message, *args, **kwargs)\n",
    "        \n",
    "    def CATCH(self, name, message, subscriber):\n",
    "        self.system.CATCH(name, message, subscriber)\n",
    "        \n",
    "    def __getitem__(self, module_name):\n",
    "        return self.system.modules[module_name]\n",
    "        \n",
    "class System():\n",
    "    def __init__(self, **kwargs):\n",
    "        self.modules = {}\n",
    "        self.subscribers = defaultdict(list)\n",
    "        for name, module in kwargs.items():\n",
    "            self.modules[name] = module(name, self)\n",
    "    \n",
    "    def SEND(self, name, message, *args, **kwargs):\n",
    "        for subscriber in self.subscribers[(name, message)]:\n",
    "            subscriber(*args, **kwargs)\n",
    "            \n",
    "    def CATCH(self, name, message, subscriber):\n",
    "        self.subscribers[(name, message)].append(subscriber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ПОЛНОЦЕННЫЙ ТЕСТ\n",
    "def Runner(parclass):    \n",
    "    class Runner(parclass):\n",
    "        def act(self, s):\n",
    "            return 42\n",
    "        \n",
    "        def run(self):\n",
    "            self.EMIT(\"transition\", self.act(0))\n",
    "    return Runner\n",
    "\n",
    "def Replay(parclass, runner):        \n",
    "    class Replay(parclass):\n",
    "        def __init__(self, name, system):\n",
    "            super().__init__(name, system)\n",
    "            self.CATCH(runner, \"transition\", self.see)\n",
    "\n",
    "        def see(self, a):\n",
    "            print(\"Replay catched \", a)\n",
    "            self.EMIT(\"batch\", a / 2)\n",
    "    return Replay\n",
    "\n",
    "def Network(parclass):\n",
    "    class Network(parclass):\n",
    "        pass\n",
    "    return Network\n",
    "\n",
    "def DQN(parclass, replay, network):\n",
    "    class DQN(parclass):\n",
    "        def __init__(self, name, system):\n",
    "            super().__init__(name, system)\n",
    "            self.CATCH(replay, \"batch\", self.batch)\n",
    "            \n",
    "        def loss(self, b):\n",
    "            return b\n",
    "            \n",
    "        def batch(self, b):\n",
    "            print(\"batch: \", self.loss(b))\n",
    "    return DQN\n",
    "\n",
    "def Target(parclass):\n",
    "    class Target(parclass):\n",
    "        def loss(self, b):\n",
    "            return b / 2\n",
    "    return Target\n",
    "\n",
    "def eGreedy(parclass, dqn):\n",
    "    class eGreedy(parclass):\n",
    "        def act(self, s):\n",
    "            return self[dqn].loss(s)\n",
    "    return eGreedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = Runner(RLmodule)\n",
    "replay = Replay(RLmodule, runner=\"runner\")\n",
    "network = Network(RLmodule)\n",
    "dqn = DQN(RLmodule, replay=\"replay\", network=\"network\")\n",
    "dqn = Target(dqn)\n",
    "runner = eGreedy(runner, dqn=\"dqn\")\n",
    "\n",
    "system = System(\n",
    "    runner = runner,\n",
    "    replay = replay,\n",
    "    network = network,\n",
    "    dqn = dqn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay catched  0.0\n",
      "batch:  0.0\n"
     ]
    }
   ],
   "source": [
    "system.modules['runner'].run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Выводы:** ну не знаю, но этот вариант пока, кажется, лучший."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ещё попытка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLmodule():\n",
    "    def __init__(self, name, system):\n",
    "        self.system = system\n",
    "        self.system.modules[name] = self\n",
    "        self.name = name\n",
    "        \n",
    "    def __getitem__(self, module_name):\n",
    "        return self.system.modules[module_name]\n",
    "        \n",
    "class System():\n",
    "    def __init__(self, **kwargs):\n",
    "        self.modules = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ПОЛНОЦЕННЫЙ ТЕСТ\n",
    "class Runner(RLmodule):\n",
    "    def __init__(self, name, system, policy=None, listeners=[]):\n",
    "        super().__init__(name, system)\n",
    "        self.policy = name if policy is None else policy\n",
    "        self.listeners = listeners\n",
    "        \n",
    "    #def add_listener(self, module):\n",
    "    #    self.listeners.append(module)\n",
    "    #    return module\n",
    "    \n",
    "    def act(self, s):\n",
    "        return 0\n",
    "\n",
    "    def run(self):\n",
    "        s = 42\n",
    "        a = self[self.policy].act(s)\n",
    "        for listener in self.listeners:\n",
    "            self[listener].see(a)\n",
    "    \n",
    "class Replay(RLmodule):\n",
    "    def __init__(self, name, system):\n",
    "        super().__init__(name, system)\n",
    "\n",
    "    def see(self, a):\n",
    "        self.a = a\n",
    "        print(\"Replay catched \", a)\n",
    "        \n",
    "    def sample(self):\n",
    "        return self.a + 1\n",
    "    \n",
    "class BatchSampler(RLmodule):\n",
    "    def __init__(self, name, system, replay, listeners=[]):\n",
    "        super().__init__(name, system)\n",
    "        self.replay = replay\n",
    "        self.listeners = listeners\n",
    "        \n",
    "#     def add_listener(self, module):\n",
    "#         self.listeners.append(module)\n",
    "#         return module\n",
    "        \n",
    "    def see(self, a):\n",
    "        batch = self[self.replay].sample()\n",
    "        print(\"Generated batch: \", batch)\n",
    "        for listener in self.listeners:\n",
    "            self[listener].process_batch(batch)\n",
    "\n",
    "class Network(RLmodule): \n",
    "    def __init__(self, name, system):\n",
    "        super().__init__(name, system)\n",
    "        self.heads = []\n",
    "        \n",
    "    def add_head(self, module):\n",
    "        self.heads.append(module)\n",
    "        return module\n",
    "        \n",
    "    def process_batch(self, batch):\n",
    "        loss = 0\n",
    "        for head in self.heads:\n",
    "            loss += head.loss(batch)\n",
    "        print(\"Loss: \", loss)\n",
    "\n",
    "class DQN(RLmodule):\n",
    "    def __init__(self, name, system, evaluator=None):\n",
    "        super().__init__(name, system)\n",
    "        self.evaluator = name if evaluator is None else evaluator\n",
    "    \n",
    "    def act(self, s):\n",
    "        return s\n",
    "    \n",
    "    def evaluate(self, batch):\n",
    "        return batch\n",
    "    \n",
    "    def loss(self, batch):\n",
    "        return self[self.evaluator].evaluate(batch)\n",
    "\n",
    "class Target(RLmodule):\n",
    "    def __init__(self, name, system, frozen_network):\n",
    "        super().__init__(name, system)\n",
    "        self.frozen_network = frozen_network\n",
    "    \n",
    "    def see(self, a):\n",
    "        print(\"target network updated\")\n",
    "    \n",
    "    def evaluate(self, b):\n",
    "        return b / 2\n",
    "\n",
    "class eGreedy(RLmodule):\n",
    "    def __init__(self, name, system, greedy_policy):\n",
    "        super().__init__(name, system)\n",
    "        self.greedy_policy = greedy_policy\n",
    "    \n",
    "    def act(self, s):\n",
    "        return self[self.greedy_policy].act(s) * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay catched  420\n",
      "Generated batch:  421\n",
      "Loss:  210.5\n",
      "target network updated\n"
     ]
    }
   ],
   "source": [
    "system  = System()\n",
    "runner  = Runner(\"runner\", system, policy=\"eGreedy\", listeners=[\"replay\", \"sampler\", \"target\"])\n",
    "replay  = Replay(\"replay\", system)\n",
    "sampler = BatchSampler(\"sampler\", system, replay=\"replay\", listeners=[\"network\"])\n",
    "network = Network(\"network\", system, heads=[\"q_head\"], losses=[\"dqn\"])\n",
    "q_head  = QHead(\"q_head\", system)\n",
    "dqn     = DQN(\"dqn\", system, evaluator=\"target\")\n",
    "target  = Target(\"target\", system, frozen_network=\"network\", head=\"q_head\")\n",
    "policy  = eGreedy(\"eGreedy\", system, greedy_policy=\"dqn\")\n",
    "runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кажись, что-то наклёвывается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'runner': <__main__.Runner at 0x1bf288e00f0>,\n",
       " 'replay': <__main__.Replay at 0x1bf288e0358>,\n",
       " 'sampler': <__main__.BatchSampler at 0x1bf288e0390>,\n",
       " 'network': <__main__.Network at 0x1bf26d773c8>,\n",
       " 'dqn': <__main__.DQN at 0x1bf26d77748>,\n",
       " 'target': <__main__.Target at 0x1bf2896d390>,\n",
       " 'eGreedy': <__main__.eGreedy at 0x1bf2896d9b0>}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system.modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# СНОВА ВАРИАНТ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# СКРЫТЫЕ ВНУТРЕННОСТИ АРХИТЕКТУРЫ:\n",
    "# всё ещё отсутствуют!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Runner():\n",
    "    class Runner():\n",
    "        def __init__(self):\n",
    "            self.storage = {}\n",
    "        \n",
    "        def act(self, s):\n",
    "            return 0\n",
    "        \n",
    "        def see(self, a):\n",
    "            pass\n",
    "        \n",
    "        def learn(self, a):\n",
    "            a = self.act(42)\n",
    "            self.see(a)\n",
    "            \n",
    "        def optimize(self, nname, a):\n",
    "            pass\n",
    "    return Runner\n",
    "\n",
    "def Replay(parclass):\n",
    "    class Replay(parclass):\n",
    "        def process_batch(self, a):\n",
    "            pass\n",
    "        \n",
    "        def see(self, a):\n",
    "            super().see(a)\n",
    "            self.process_batch(a)\n",
    "    return Replay\n",
    "\n",
    "def Network(parclass, name):\n",
    "    class Network(parclass):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            assert name not in self.storage, name\n",
    "            self.storage[name] = 0\n",
    "            print(\"added: \", name)\n",
    "        \n",
    "        def optimize(self, nname, a):\n",
    "            super().optimize(nname, a)\n",
    "            if nname == name:\n",
    "                print(nname, \" loss is \", self.storage[name])\n",
    "        \n",
    "        def process_batch(self, a):\n",
    "            super().process_batch(a)\n",
    "            print(\"Launching optimization with \", name)\n",
    "            self.optimize(name, a)\n",
    "    return Network\n",
    "\n",
    "def DQN(parclass, use_network):\n",
    "    class DQN(parclass):\n",
    "        def loss(self, a):\n",
    "            return a - 1\n",
    "        \n",
    "        def optimize(self, nname, a):\n",
    "            if nname == use_network:\n",
    "                self.storage[nname] += self.loss(a)\n",
    "            super().optimize(nname, a)\n",
    "    return DQN\n",
    "\n",
    "def Target(parclass):\n",
    "    class Target(parclass):\n",
    "        def loss(self, a):\n",
    "            return a - 10\n",
    "    return Target\n",
    "\n",
    "def eGreedy(parclass):\n",
    "    class eGreedy(parclass):\n",
    "        def act(self, s):\n",
    "            return 42\n",
    "    return eGreedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added:  hi!\n",
      "added:  weell\n",
      "Launching optimization with  hi!\n",
      "hi!  loss is  32\n",
      "Launching optimization with  weell\n",
      "weell  loss is  0\n"
     ]
    }
   ],
   "source": [
    "Agent = Runner()\n",
    "Agent = Replay(Agent)\n",
    "Agent = Network(Agent, \"hi!\")\n",
    "Agent = DQN(Agent, \"hi!\")\n",
    "Agent = Target(Agent)\n",
    "Agent = eGreedy(Agent)\n",
    "Agent = Network(Agent, \"weell\")\n",
    "\n",
    "agent = Agent()\n",
    "agent.learn(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hi!': 32, 'weell': 0}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
